{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYvFK9QpZLCRXG2HoN1685",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezve66/Genetic-Diversity/blob/main/SSR_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeRTvTYR8RMw"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üìä SSR Genetic Diversity and Differentiation Analysis\n",
        "# Google Colab Ready Script\n",
        "# ============================================\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# üß¨ Core Genetic Diversity Functions\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Major Allele Frequency (MAF)\n",
        "def major_allele_frequency(marker_data):\n",
        "    unique, counts = np.unique(marker_data, return_counts=True)\n",
        "    major_allele_freq = max(counts) / len(marker_data)\n",
        "    return major_allele_freq\n",
        "\n",
        "# Number of Alleles (Na)\n",
        "def number_of_alleles(marker_data):\n",
        "    unique_alleles = np.unique(marker_data)\n",
        "    return len(unique_alleles)\n",
        "\n",
        "# Polymorphic Information Content (PIC)\n",
        "def polymorphic_information_content(marker_data):\n",
        "    unique, counts = np.unique(marker_data, return_counts=True)\n",
        "    p_values = counts / len(marker_data)\n",
        "    pic = 1 - sum(p_values ** 2)\n",
        "    return pic\n",
        "\n",
        "# Observed Heterozygosity (Obs_Het)\n",
        "def observed_heterozygosity(marker_data):\n",
        "    obs_het = np.sum(marker_data == 1) / len(marker_data)\n",
        "    return obs_het\n",
        "\n",
        "# Expected Heterozygosity (Exp_Het)\n",
        "def expected_heterozygosity(marker_data):\n",
        "    unique, counts = np.unique(marker_data, return_counts=True)\n",
        "    p_values = counts / len(marker_data)\n",
        "    exp_het = 1 - sum(p_values ** 2)\n",
        "    return exp_het\n",
        "\n",
        "# Gene Diversity (He)\n",
        "def gene_diversity(marker_data):\n",
        "    return expected_heterozygosity(marker_data)\n",
        "\n",
        "# Shannon Index (I)\n",
        "def shannon_index(marker_data):\n",
        "    unique, counts = np.unique(marker_data, return_counts=True)\n",
        "    proportions = counts / len(marker_data)\n",
        "    shannon = -np.sum(proportions * np.log(proportions))\n",
        "    return shannon\n",
        "\n",
        "# Simpson‚Äôs Index (L)\n",
        "def simpson_index(marker_data):\n",
        "    unique, counts = np.unique(marker_data, return_counts=True)\n",
        "    proportions = counts / len(marker_data)\n",
        "    simpson = 1 - np.sum(proportions ** 2)\n",
        "    return simpson\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# üåç Genetic Differentiation (Fst and Nm)\n",
        "# ----------------------------------------------------------\n",
        "def calculate_fst_nm(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    fst_nm_params = []\n",
        "\n",
        "    for marker in data.columns[1:]:  # Skip first column (sample names)\n",
        "        marker_data = data[marker].values\n",
        "\n",
        "        # Calculate allele frequency\n",
        "        allele_freqs = marker_data.mean() / 2\n",
        "\n",
        "        # Variance calculations\n",
        "        variance_total = np.var(marker_data)\n",
        "        variance_between = allele_freqs * (1 - allele_freqs)\n",
        "\n",
        "        # Fst and Nm\n",
        "        fst_value = variance_between / variance_total if variance_total != 0 else np.nan\n",
        "        nm_value = 0.25 * (1 - fst_value) / fst_value if fst_value not in [0, np.nan] else np.nan\n",
        "\n",
        "        fst_nm_params.append({\n",
        "            \"Marker\": marker,\n",
        "            \"Fst\": fst_value,\n",
        "            \"Nm\": nm_value\n",
        "        })\n",
        "\n",
        "    fst_nm_df = pd.DataFrame(fst_nm_params)\n",
        "    return fst_nm_df\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# üßÆ Full SSR Data Analysis Pipeline\n",
        "# ----------------------------------------------------------\n",
        "def analyze_ssr_data(file_path):\n",
        "    data = pd.read_excel(file_path)\n",
        "    genetic_params = []\n",
        "\n",
        "    for marker in data.columns[1:]:  # Skip first column (sample names)\n",
        "        marker_data = data[marker].values\n",
        "        genetic_params.append({\n",
        "            \"Marker\": marker,\n",
        "            \"MAF\": major_allele_frequency(marker_data),\n",
        "            \"Na\": number_of_alleles(marker_data),\n",
        "            \"PIC\": polymorphic_information_content(marker_data),\n",
        "            \"Obs_Het\": observed_heterozygosity(marker_data),\n",
        "            \"Exp_Het\": expected_heterozygosity(marker_data),\n",
        "            \"He\": gene_diversity(marker_data),\n",
        "            \"Shannon_I\": shannon_index(marker_data),\n",
        "            \"Simpson_L\": simpson_index(marker_data)\n",
        "        })\n",
        "\n",
        "    genetic_params_df = pd.DataFrame(genetic_params)\n",
        "    fst_nm_df = calculate_fst_nm(file_path)\n",
        "\n",
        "    return genetic_params_df, fst_nm_df\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# üöÄ Main Function for Google Colab Execution\n",
        "# ----------------------------------------------------------\n",
        "def main():\n",
        "    print(\"üìÅ Please upload your SSR genotype Excel file (.xlsx)\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "\n",
        "    print(\"\\nüîÑ Processing genetic diversity analysis...\")\n",
        "    genetic_analysis_result, fst_nm_result = analyze_ssr_data(file_path)\n",
        "\n",
        "    print(\"\\n‚úÖ Genetic Diversity Analysis Results:\")\n",
        "    display(genetic_analysis_result)\n",
        "\n",
        "    print(\"\\n‚úÖ Genetic Differentiation (Fst) and Gene Flow (Nm) Results:\")\n",
        "    display(fst_nm_result)\n",
        "\n",
        "    # Save outputs as CSV files\n",
        "    genetic_analysis_result.to_csv(\"genetic_analysis_result.csv\", index=False)\n",
        "    fst_nm_result.to_csv(\"fst_nm_result.csv\", index=False)\n",
        "\n",
        "    print(\"\\nüíæ Download your results:\")\n",
        "    files.download(\"genetic_analysis_result.csv\")\n",
        "    files.download(\"fst_nm_result.csv\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ‚ñ∂Ô∏è Run the Analysis\n",
        "# ----------------------------------------------------------\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# üåæ SSR Marker Full Analysis Pipeline (Colab Ready)\n",
        "# ================================================\n",
        "\n",
        "# 1Ô∏è‚É£ Install required packages\n",
        "!pip install --quiet scikit-bio openpyxl xlsxwriter seaborn statsmodels\n",
        "\n",
        "# 2Ô∏è‚É£ Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import MDS\n",
        "from scipy.stats import chi2_contingency\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from skbio import DistanceMatrix\n",
        "from skbio.tree import nj\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3Ô∏è‚É£ Upload SSR Excel or CSV file\n",
        "# ------------------------------------------------\n",
        "print(\"üì§ Upload your SSR Excel or CSV file:\")\n",
        "uploaded = files.upload()\n",
        "file_path = Path(list(uploaded.keys())[0])\n",
        "print(f\"‚úÖ Uploaded file: {file_path}\")\n",
        "\n",
        "if file_path.suffix in [\".xlsx\", \".xls\"]:\n",
        "    df_raw = pd.read_excel(file_path)\n",
        "else:\n",
        "    df_raw = pd.read_csv(file_path)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4Ô∏è‚É£ Preprocess: Detect sample IDs and binarize\n",
        "# ------------------------------------------------\n",
        "df = df_raw.copy()\n",
        "if df.shape[1] >= 2 and (df.iloc[:, 0].dtype == object or df.iloc[:, 0].nunique() == df.shape[0]):\n",
        "    df = df.set_index(df.columns[0])\n",
        "else:\n",
        "    df.index = [f\"Sample_{i+1}\" for i in range(df.shape[0])]\n",
        "\n",
        "def to_binary(v):\n",
        "    if pd.isna(v):\n",
        "        return 0\n",
        "    try:\n",
        "        fv = float(v)\n",
        "        return 1 if fv > 0 else 0\n",
        "    except Exception:\n",
        "        s = str(v).strip().lower()\n",
        "        if s in {\"1\",\"yes\",\"y\",\"+\",\"present\",\"p\",\"true\",\"t\"}:\n",
        "            return 1\n",
        "        if s in {\"0\",\"no\",\"n\",\"-\",\"absent\",\"a\",\"false\",\"f\"}:\n",
        "            return 0\n",
        "        return 1\n",
        "\n",
        "binary = df.applymap(to_binary).astype(int)\n",
        "binary = binary.loc[(binary.sum(axis=1) > 0), (binary.sum(axis=0) > 0)]\n",
        "print(\"‚úÖ Cleaned binary SSR matrix (Samples √ó Markers):\")\n",
        "display(binary.head())\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "binary.to_csv(\"results/SSR_binary_matrix.csv\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5Ô∏è‚É£ Jaccard Distance (samples)\n",
        "# ------------------------------------------------\n",
        "print(\"üîπ Computing Jaccard distance between samples...\")\n",
        "X = binary.values.astype(bool)\n",
        "sample_labels = list(binary.index.astype(str))\n",
        "\n",
        "condensed = pdist(X, metric=\"jaccard\")\n",
        "dist_mat = squareform(condensed)\n",
        "dist_df = pd.DataFrame(dist_mat, index=sample_labels, columns=sample_labels)\n",
        "dist_df.to_csv(\"results/Jaccard_distance_matrix.csv\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6Ô∏è‚É£ UPGMA Tree + Save .newick\n",
        "# ------------------------------------------------\n",
        "Z_avg = linkage(condensed, method=\"average\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(Z_avg, labels=sample_labels, leaf_rotation=90)\n",
        "plt.title(\"UPGMA Dendrogram (Sample-based)\")\n",
        "plt.ylabel(\"Jaccard Distance\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/UPGMA_dendrogram.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "def linkage_to_newick(Z, labels):\n",
        "    n = len(labels)\n",
        "    nodes = {i: (labels[i], 0.0) for i in range(n)}\n",
        "    next_id = n\n",
        "    for (a, b, dist, _) in Z:\n",
        "        a, b = int(a), int(b)\n",
        "        name_a, h_a = nodes[a]\n",
        "        name_b, h_b = nodes[b]\n",
        "        h = dist / 2.0\n",
        "        new_name = f\"({name_a}:{max(h-h_a,0):.5f},{name_b}:{max(h-h_b,0):.5f})\"\n",
        "        nodes[next_id] = (new_name, h)\n",
        "        next_id += 1\n",
        "    return nodes[next_id-1][0] + \";\"\n",
        "\n",
        "upgma_newick = linkage_to_newick(Z_avg, sample_labels)\n",
        "with open(\"results/UPGMA_tree.newick\", \"w\") as f:\n",
        "    f.write(upgma_newick)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7Ô∏è‚É£ Neighbor-Joining (NJ) Tree + Save .newick\n",
        "# ------------------------------------------------\n",
        "dm = DistanceMatrix(dist_mat, ids=sample_labels)\n",
        "nj_tree = nj(dm)\n",
        "\n",
        "# Version-safe NJ tree writing\n",
        "with open(\"results/NJ_tree.newick\", \"w\") as f:\n",
        "    nj_tree.write(f, format=\"newick\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8Ô∏è‚É£ Marker Diversity Statistics\n",
        "# ------------------------------------------------\n",
        "freqs = binary.mean(axis=0)\n",
        "marker_stats = pd.DataFrame({\n",
        "    \"Marker\": binary.columns,\n",
        "    \"Presence_Count\": binary.sum(axis=0),\n",
        "    \"Absence_Count\": binary.shape[0] - binary.sum(axis=0),\n",
        "    \"Allele_Freq\": freqs,\n",
        "    \"Shannon_Index\": [-p*np.log2(p)-(1-p)*np.log2(1-p) if 0<p<1 else 0 for p in freqs],\n",
        "    \"Simpson_Index\": [1-(p**2+(1-p)**2) for p in freqs],\n",
        "    \"PIC_approx\": 2*freqs*(1-freqs)\n",
        "}).set_index(\"Marker\")\n",
        "marker_stats.to_csv(\"results/Marker_statistics.csv\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9Ô∏è‚É£ Linkage Disequilibrium (LD)\n",
        "# ------------------------------------------------\n",
        "results=[]\n",
        "markers=list(binary.columns)\n",
        "for i in range(len(markers)):\n",
        "    for j in range(i+1,len(markers)):\n",
        "        m1,m2=markers[i],markers[j]\n",
        "        ctab=pd.crosstab(binary[m1],binary[m2])\n",
        "        if ctab.shape!=(2,2): continue\n",
        "        chi2,p,_,_=chi2_contingency(ctab,correction=False)\n",
        "        results.append((m1,m2,chi2,p))\n",
        "ld_df=pd.DataFrame(results,columns=[\"Marker1\",\"Marker2\",\"Chi2\",\"Pvalue\"])\n",
        "if len(ld_df)>0:\n",
        "    _, p_bon, _, _ = multipletests(ld_df[\"Pvalue\"], method=\"bonferroni\")\n",
        "    ld_df[\"Pvalue_Bonferroni\"] = p_bon\n",
        "ld_df.to_csv(\"results/LD_results.csv\",index=False)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# üîü PCA & MDS (Sample-Based)\n",
        "# ------------------------------------------------\n",
        "Xs = StandardScaler().fit_transform(binary.fillna(0).values)\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(Xs)\n",
        "pca_df = pd.DataFrame(pcs, index=binary.index, columns=[\"PC1\",\"PC2\"])\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], s=80)\n",
        "for i,txt in enumerate(pca_df.index):\n",
        "    plt.annotate(txt,(pca_df[\"PC1\"].iat[i],pca_df[\"PC2\"].iat[i]),fontsize=8)\n",
        "plt.title(\"PCA of SSR Genotypes (Samples)\")\n",
        "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
        "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/PCA_plot.png\",dpi=300)\n",
        "plt.close()\n",
        "\n",
        "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
        "coords = mds.fit_transform(dist_mat)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(coords[:,0],coords[:,1],s=80)\n",
        "for i,txt in enumerate(sample_labels):\n",
        "    plt.annotate(txt,(coords[i,0],coords[i,1]),fontsize=8)\n",
        "plt.title(\"MDS of Jaccard Distances (Samples)\")\n",
        "plt.xlabel(\"Dimension 1\"); plt.ylabel(\"Dimension 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/MDS_plot.png\",dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 11Ô∏è‚É£ AMOVA-like Partition (Sample-Based)\n",
        "# ------------------------------------------------\n",
        "row_medians = np.median(dist_mat, axis=1)\n",
        "median_all = np.median(dist_mat)\n",
        "g1 = np.where(row_medians < median_all)[0]\n",
        "g2 = np.where(row_medians >= median_all)[0]\n",
        "\n",
        "def mean_upper(mat, idx):\n",
        "    if len(idx)<=1: return np.nan\n",
        "    sub = mat[np.ix_(idx,idx)]\n",
        "    return np.nanmean(sub[np.triu_indices_from(sub,k=1)])\n",
        "\n",
        "within1 = mean_upper(dist_mat,g1)\n",
        "within2 = mean_upper(dist_mat,g2)\n",
        "between = np.nanmean(dist_mat[np.ix_(g1,g2)])\n",
        "total = np.nanmean(dist_mat[np.triu_indices_from(dist_mat,k=1)])\n",
        "pct_between = between/total if total>0 else np.nan\n",
        "\n",
        "amova_df=pd.DataFrame({\n",
        "    \"Group\":[\"G1\",\"G2\",\"Between\",\"Total\"],\n",
        "    \"Mean_Dist\":[within1,within2,between,total],\n",
        "    \"Pct_Between\":[np.nan,np.nan,pct_between,np.nan]\n",
        "})\n",
        "amova_df.to_csv(\"results/AMOVA_summary.csv\",index=False)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 12Ô∏è‚É£ Heatmap & Stacked Bar\n",
        "# ------------------------------------------------\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(binary.T, cmap=\"coolwarm\", cbar_kws={\"label\":\"Allele Presence (0/1)\"})\n",
        "plt.title(\"SSR Marker Heatmap (Samples √ó Markers)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/Heatmap.png\",dpi=300)\n",
        "plt.close()\n",
        "\n",
        "prop = binary.apply(lambda x: x.value_counts(normalize=True), axis=0).T.fillna(0)\n",
        "prop[[0,1]].plot(kind='bar', stacked=True, figsize=(12,6))\n",
        "plt.title(\"Stacked Bar of SSR Markers\")\n",
        "plt.xlabel(\"Markers\"); plt.ylabel(\"Proportion\")\n",
        "plt.legend([\"Absent (0)\",\"Present (1)\"])\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/Stacked_Bar.png\",dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 13Ô∏è‚É£ Save Everything to a Single Excel Workbook\n",
        "# ------------------------------------------------\n",
        "excel_out = \"results/SSR_analysis_results.xlsx\"\n",
        "with pd.ExcelWriter(excel_out, engine=\"xlsxwriter\") as writer:\n",
        "    binary.to_excel(writer, sheet_name=\"Binary_Matrix\")\n",
        "    dist_df.to_excel(writer, sheet_name=\"Jaccard_Dist\")\n",
        "    marker_stats.to_excel(writer, sheet_name=\"Marker_Stats\")\n",
        "    ld_df.to_excel(writer, sheet_name=\"LD_Results\", index=False)\n",
        "    pca_df.to_excel(writer, sheet_name=\"PCA\")\n",
        "    amova_df.to_excel(writer, sheet_name=\"AMOVA\")\n",
        "\n",
        "print(\"‚úÖ All analysis complete! Files saved in 'results/' folder.\")\n",
        "files.download(excel_out)\n",
        "files.download(\"results/UPGMA_tree.newick\")\n",
        "files.download(\"results/NJ_tree.newick\")\n"
      ],
      "metadata": {
        "id": "0VUqxF5ABCti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- üìä Refined Heatmap and Network Graph for Journal Submission ---\n",
        "# ‚úÖ Works with both CSV and Excel files directly in Google Colab\n",
        "\n",
        "# üì¶ Install required packages\n",
        "!pip install networkx matplotlib pandas openpyxl seaborn --quiet\n",
        "\n",
        "# üìÅ Step 1: Upload your file\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"üì§ Please upload your Jaccard distance matrix file (.csv or .xlsx):\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # take the first uploaded file\n",
        "print(f\"‚úÖ File uploaded: {file_path}\")\n",
        "\n",
        "# --- Step 2: Read the matrix file automatically ---\n",
        "if file_path.endswith(\".csv\"):\n",
        "    df = pd.read_csv(file_path, index_col=0)\n",
        "elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "    df = pd.read_excel(file_path, index_col=0)\n",
        "else:\n",
        "    raise ValueError(\"‚ùå Unsupported file format. Please upload a .csv or .xlsx file.\")\n",
        "\n",
        "print(\"\\nüìÑ Data shape:\", df.shape)\n",
        "print(\"üìä Preview:\")\n",
        "display(df.head())\n",
        "\n",
        "# --- Step 3: Convert Jaccard Distance ‚Üí Similarity ---\n",
        "# Distance: 0 = identical, 1 = completely different\n",
        "# Similarity = 1 - distance\n",
        "sim_df = 1 - df\n",
        "\n",
        "# --- Step 4: Create and display Heatmap ---\n",
        "heatmap_path = \"jaccard_heatmap.png\"\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(sim_df, annot=False, cmap=\"YlGnBu\", cbar_kws={'label': 'Jaccard Distance'},\n",
        "            linewidths=0.5, linecolor='gray', xticklabels=df.index, yticklabels=df.index)\n",
        "\n",
        "# Add title and axis labels\n",
        "plt.title('Jaccard Distance Matrix of Entities', fontsize=16, weight='bold')\n",
        "plt.xlabel('Sample ID', fontsize=12, weight='bold')\n",
        "plt.ylabel('Sample ID', fontsize=12, weight='bold')\n",
        "\n",
        "# Rotate the x-axis labels for better readability\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "\n",
        "# Save the heatmap image\n",
        "plt.tight_layout()\n",
        "plt.savefig(heatmap_path, dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()  # Close the plot to free up memory\n",
        "print(f\"‚úÖ Heatmap image saved as {heatmap_path}\")\n",
        "\n",
        "# --- Step 5: Convert similarity matrix to edge list ---\n",
        "edges = sim_df.stack().reset_index()\n",
        "edges.columns = [\"u\", \"v\", \"similarity\"]\n",
        "\n",
        "# Remove self-loops and duplicate symmetric pairs\n",
        "edges = edges[edges[\"u\"] != edges[\"v\"]]\n",
        "edges = edges.drop_duplicates(subset=[\"u\", \"v\"])\n",
        "\n",
        "# --- Step 6: Filter by similarity threshold ---\n",
        "threshold = 0.5  # üëà Adjust this value (0.0‚Äì1.0)\n",
        "edges_filtered = edges[edges[\"similarity\"] > threshold]\n",
        "print(f\"üìâ Edges retained with similarity > {threshold}: {edges_filtered.shape[0]}\")\n",
        "\n",
        "# --- Step 7: Build network graph ---\n",
        "G = nx.Graph()\n",
        "for _, row in edges_filtered.iterrows():\n",
        "    G.add_edge(str(row[\"u\"]), str(row[\"v\"]), weight=float(row[\"similarity\"]))\n",
        "\n",
        "print(\"\\nüì¶ Graph Summary:\")\n",
        "print(f\"   ‚Ä¢ Nodes: {G.number_of_nodes()}\")\n",
        "print(f\"   ‚Ä¢ Edges: {G.number_of_edges()}\")\n",
        "\n",
        "if G.number_of_nodes() == 0:\n",
        "    raise ValueError(\"‚ùå No edges above threshold. Try lowering the threshold value.\")\n",
        "\n",
        "# --- Step 8: Prepare visualization parameters ---\n",
        "deg = dict(G.degree())\n",
        "maxdeg = max(deg.values()) if deg else 1\n",
        "node_sizes = [600 + (deg[n] / maxdeg) * 1400 for n in G.nodes()]\n",
        "\n",
        "weights = nx.get_edge_attributes(G, \"weight\")\n",
        "if len(weights) > 0:\n",
        "    maxw = max(weights.values())\n",
        "    edge_widths = [max(0.6, (w / maxw) * 6) for w in weights.values()]\n",
        "else:\n",
        "    edge_widths = [1 for _ in G.edges()]\n",
        "\n",
        "# Layout options: spring_layout, kamada_kawai_layout, circular_layout, etc.\n",
        "pos = nx.spring_layout(G, seed=42, k=0.4, iterations=300)\n",
        "\n",
        "# --- Step 9: Draw the network graph ---\n",
        "network_graph_path = \"jaccard_network_refined.png\"\n",
        "plt.figure(figsize=(14, 14))\n",
        "\n",
        "# Refined node and edge drawing\n",
        "nx.draw_networkx_nodes(\n",
        "    G, pos,\n",
        "    node_size=node_sizes,\n",
        "    node_color=\"skyblue\",  # Soft color for nodes\n",
        "    edgecolors=\"black\",  # Black edges around nodes\n",
        "    linewidths=1.5  # Thicker borders around nodes\n",
        ")\n",
        "\n",
        "# Refined labels with bold font\n",
        "nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\", font_color=\"black\")\n",
        "\n",
        "# Refined edges with width proportional to similarity\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, edge_color=\"gray\")\n",
        "\n",
        "# Edge labels with similarity values\n",
        "edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in G.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7, font_weight=\"bold\")\n",
        "\n",
        "# Title and layout adjustments for journal style\n",
        "plt.title(\"üìä Genotype Network Based on Jaccard Similarity\", fontsize=18, fontweight='bold')\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the refined network graph image\n",
        "plt.savefig(network_graph_path, dpi=600, bbox_inches=\"tight\")\n",
        "plt.close()  # Close the plot to free up memory\n",
        "print(f\"‚úÖ Refined network graph image saved as {network_graph_path}\")\n",
        "\n",
        "# --- Step 10: Download the refined network graph and heatmap ---\n",
        "from google.colab import files\n",
        "files.download(heatmap_path)\n",
        "files.download(network_graph_path)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4lw4cw7fIKt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Install necessary packages (run once)\n",
        "!pip install networkx matplotlib pandas --quiet\n",
        "\n",
        "# üìÅ Step 1: Upload your CSV or Excel file\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "csv_path = list(uploaded.keys())[0]  # Take the first uploaded file\n",
        "print(\"‚úÖ File uploaded:\", csv_path)\n",
        "\n",
        "# Step 2: Read the CSV/Excel file\n",
        "df = pd.read_csv(csv_path)  # Change to pd.read_excel if your file is Excel\n",
        "print(\"\\nüìÑ File Shape:\", df.shape)\n",
        "print(\"üìä Columns:\", list(df.columns))\n",
        "display(df.head(10))\n",
        "\n",
        "# --- Step 3: Infer Edge List from the Dataset ---\n",
        "def infer_edges_from_df(df):\n",
        "    \"\"\"\n",
        "    This function infers the edge list from various formats in the dataset.\n",
        "    It checks for different structures such as an edge list, adjacency matrix, or a simple 2-column edge list.\n",
        "    \"\"\"\n",
        "    # Identify object and numeric columns\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\" or df[c].dtype.name == \"string\"]\n",
        "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "    # Case 1: Edge list (node1, node2, weight)\n",
        "    if len(obj_cols) >= 2 and len(num_cols) >= 1:\n",
        "        node1, node2, weight = obj_cols[0], obj_cols[1], num_cols[0]\n",
        "        edges = df[[node1, node2, weight]].dropna().rename(columns={node1:\"u\", node2:\"v\", weight:\"w\"})\n",
        "        return edges[[\"u\",\"v\",\"w\"]]\n",
        "\n",
        "    # Case 2: Two columns ‚Üí unweighted edge list\n",
        "    if df.shape[1] == 2:\n",
        "        ucol, vcol = df.columns[0], df.columns[1]\n",
        "        edges = df.rename(columns={ucol:\"u\", vcol:\"v\"})\n",
        "        edges[\"w\"] = 1.0  # Assign weight 1.0 to all edges\n",
        "        return edges[[\"u\",\"v\",\"w\"]]\n",
        "\n",
        "    # Case 3: Adjacency matrix\n",
        "    if df.shape[0] == df.shape[1] or (pd.api.types.is_string_dtype(df.iloc[:,0]) and df.shape[1] > 2):\n",
        "        if not pd.api.types.is_numeric_dtype(df.iloc[:,0]):\n",
        "            df2 = df.set_index(df.columns[0])\n",
        "            df_num = df2.apply(pd.to_numeric, errors=\"coerce\")\n",
        "            stacked = df_num.stack().reset_index()\n",
        "            stacked.columns = [\"u\",\"v\",\"w\"]\n",
        "            stacked = stacked.dropna(subset=[\"w\"])\n",
        "            stacked = stacked[stacked[\"w\"] != 0]\n",
        "            return stacked\n",
        "\n",
        "    # Fallback: assume first three columns are u, v, w\n",
        "    if df.shape[1] >= 3:\n",
        "        edges = df.iloc[:, :3].rename(columns={df.columns[0]:\"u\", df.columns[1]:\"v\", df.columns[2]:\"w\"})\n",
        "        return edges[[\"u\",\"v\",\"w\"]].dropna()\n",
        "\n",
        "    raise ValueError(\"‚ùå Could not infer edge list format. Please check your CSV columns.\")\n",
        "\n",
        "# Get edges from dataframe\n",
        "edges = infer_edges_from_df(df)\n",
        "print(\"\\n‚úÖ Inferred edges:\", edges.shape[0])\n",
        "display(edges.head(20))\n",
        "\n",
        "# --- Step 4: Filter Edges Based on Weight Threshold ---\n",
        "threshold = 0.0  # üëà you can change this to 0.3, 0.5, etc. to filter weak edges\n",
        "edges_filtered = edges[edges[\"w\"] > threshold].copy()\n",
        "print(f\"üìâ Filtered edges > {threshold}: {edges_filtered.shape[0]}\")\n",
        "\n",
        "# --- Step 5: Build the Graph ---\n",
        "G = nx.Graph()  # Create an undirected graph\n",
        "for _, row in edges_filtered.iterrows():\n",
        "    u, v, w = str(row[\"u\"]), str(row[\"v\"]), float(row[\"w\"])\n",
        "    if u == v:  # Avoid self-loops\n",
        "        continue\n",
        "    G.add_edge(u, v, weight=w)\n",
        "\n",
        "print(\"üì¶ Graph summary:\")\n",
        "print(\"   Nodes:\", G.number_of_nodes())\n",
        "print(\"   Edges:\", G.number_of_edges())\n",
        "\n",
        "# --- Step 6: Prepare Visualization ---\n",
        "# Calculate node sizes based on degree (larger nodes have higher degrees)\n",
        "deg = dict(G.degree())\n",
        "maxdeg = max(deg.values()) if deg else 1\n",
        "node_sizes = [600 + (deg[n] / maxdeg) * 1400 for n in G.nodes()]\n",
        "\n",
        "# Calculate edge widths based on weight (thicker edges for stronger weights)\n",
        "weights = nx.get_edge_attributes(G, \"weight\")\n",
        "if len(weights) > 0:\n",
        "    maxw = max(weights.values())\n",
        "    edge_widths = [max(0.6, (w / maxw) * 6) for w in weights.values()]\n",
        "else:\n",
        "    edge_widths = [1 for _ in G.edges()]\n",
        "\n",
        "# Set positions using spring layout\n",
        "pos = nx.spring_layout(G, seed=42, k=0.4, iterations=200)\n",
        "\n",
        "# --- Step 7: Draw the Network ---\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"skyblue\", edgecolors=\"black\", linewidths=0.8)\n",
        "\n",
        "# Draw node labels (optional)\n",
        "nx.draw_networkx_labels(G, pos, font_size=9, font_weight=\"bold\")\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.9)\n",
        "\n",
        "# Draw edge labels (optional)\n",
        "edge_labels = {(u,v): f\"{d['weight']:.2f}\" for u,v,d in G.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
        "\n",
        "# Clean up visualization\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"üìä Network Plot from LD Results\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Step 8: Save and Display the Figure ---\n",
        "out_path = \"network_plot.png\"\n",
        "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")  # Save the figure\n",
        "plt.show()  # Display the figure\n",
        "\n",
        "print(\"\\n‚úÖ Saved figure as:\", out_path)\n",
        "\n",
        "# üì• Step 9: Download the figure and results\n",
        "from google.colab import files\n",
        "files.download(out_path)  # Download the network plot image\n"
      ],
      "metadata": {
        "id": "ZQm1AQxZIQhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Data setup\n",
        "data = {\n",
        "    \"Marker1\": ['tub1', 'tub1', 'tub1', 'aflR', 'aflR', 'OmtB'],\n",
        "    \"Marker2\": ['aflR', 'OmtB', 'Omt1', 'OmtB', 'Omt1', 'Omt1'],\n",
        "    \"Chi2\": [0.057057057, 5.147089947, 3.619047619, 0.088030888, 0.599099099, 5.583673469],\n",
        "    \"Pvalue\": [0.811209343, 0.023285365, 0.057121564, 0.76669556, 0.438921967, 0.018128658],\n",
        "    \"Pvalue_Bonferroni\": [1, 0.139712191, 0.342729384, 1, 1, 0.10877195]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Adjusted significance level after Bonferroni correction\n",
        "alpha_bonferroni = 0.05 / len(df)\n",
        "\n",
        "# Create a new column to check if the p-values are significant (before and after Bonferroni correction)\n",
        "df['Significant (p-value < 0.05)'] = df['Pvalue'] < 0.05\n",
        "df['Significant (Bonferroni)'] = df['Pvalue_Bonferroni'] < alpha_bonferroni\n",
        "\n",
        "# Calculate Cram√©r's V for each comparison (effect size)\n",
        "# Cram√©r's V = sqrt(Chi2 / (n * (min(k1, k2) - 1)))\n",
        "# where n = total sample size, k1 and k2 are the number of categories for each variable\n",
        "n = 100  # Placeholder for total sample size\n",
        "k1 = 2   # Assuming 2 categories for each marker (simplification)\n",
        "k2 = 2   # Same assumption for simplicity\n",
        "\n",
        "df['Cramer\\'s V'] = np.sqrt(df['Chi2'] / (n * (min(k1, k2) - 1)))\n",
        "\n",
        "# Create a heatmap for the significance of the p-values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df[['Chi2', 'Pvalue', 'Pvalue_Bonferroni']].T,\n",
        "            annot=True, fmt='.3f', cmap='coolwarm', cbar_kws={'label': 'Values'},\n",
        "            linewidths=0.5)\n",
        "plt.title(\"Heatmap of Chi2, P-value and Bonferroni-corrected P-value\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the significance results using bar plots for better understanding\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Bar plot for p-value significance\n",
        "axes[0].barh(df['Marker1'] + ' vs ' + df['Marker2'], df['Pvalue'], color=df['Significant (p-value < 0.05)'].map({True: 'green', False: 'red'}))\n",
        "axes[0].set_title(\"Significance based on p-value < 0.05\")\n",
        "axes[0].set_xlabel('P-value')\n",
        "axes[0].set_ylabel('Marker Comparisons')\n",
        "\n",
        "# Bar plot for Bonferroni-corrected p-value significance\n",
        "axes[1].barh(df['Marker1'] + ' vs ' + df['Marker2'], df['Pvalue_Bonferroni'], color=df['Significant (Bonferroni)'].map({True: 'green', False: 'red'}))\n",
        "axes[1].set_title(\"Significance based on Bonferroni-corrected p-value\")\n",
        "axes[1].set_xlabel('Bonferroni-corrected P-value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for Chi2 vs P-value for visualization of strength and significance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Chi2'], df['Pvalue'], c=df['Significant (p-value < 0.05)'].map({True: 'green', False: 'red'}),\n",
        "            label='Significant', s=100, alpha=0.7)\n",
        "plt.xlabel('Chi2')\n",
        "plt.ylabel('P-value')\n",
        "plt.title('Scatter Plot of Chi2 vs P-value')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for Chi2 vs Cram√©r's V (to visualize the effect size)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Chi2'], df['Cramer\\'s V'], c=df['Significant (p-value < 0.05)'].map({True: 'green', False: 'red'}),\n",
        "            label='Significant', s=100, alpha=0.7)\n",
        "plt.xlabel('Chi2')\n",
        "plt.ylabel(\"Cram√©r's V (Effect Size)\")\n",
        "plt.title(\"Chi2 vs Cram√©r's V (Effect Size)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q11tk7NLI7VP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}